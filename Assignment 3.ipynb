{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Printed copy due in class on November 14, 2018\n",
    "\n",
    "You may work in pairs on this assignment. You are not permitted to discuss this assignment with anyone other than your partner or the instructors.\n",
    "\n",
    "### Student 1:\n",
    "### Student 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Neural Networks by Hand\n",
    "\n",
    "## Part A\n",
    "\n",
    "Suppose we train a neural network using the ReLU activation function: \n",
    "\n",
    "$$ g(a) = \\max(a, 0). $$\n",
    "\n",
    "1. Draw the graph of $g(a)$. (Matplotlib plots are acceptable.)\n",
    "2. What is the derivative $h = g'(a)$ in terms of the input $a$?\n",
    "3. What is the derivative $h = g'(a)$ in terms of the input $h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR WORK HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Consider a neural network, where the current weights and biases are as follows. (Since this network is so simple, we omit the superscripts $w^{(m)}$ and $b^{(m)}$ that indicates which layer each weight/bias is in.)\n",
    "\n",
    "![](neural_network.png)\n",
    "\n",
    "The activation function used in the hidden layer is the ReLU activation function. \n",
    "**No activation function is used in the output layer.**\n",
    "\n",
    "Predict the output for the input ${\\bf x} = (-1, 1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR WORK HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "We are training the neural network from the previous part to minimize the loss function \n",
    "\n",
    "$$ L_i({\\bf w}, {\\bf b}) = (\\hat y_i - y_i)^2. $$\n",
    "\n",
    "You have a new training observation ${\\bf x_i} = (-1, 1)$ and $y_i = 1$. \n",
    "Use one step of (stochastic) gradient descent to update the 6 weights (from the previous part) ${\\bf w} = (w_{11}, w_{12}, w_{21}, w_{22}, w_1, w_2)$ and the 3 biases (from the previous part) ${\\bf b} = (b_1, b_2, b)$, based on this single observation. Use a learning rate of $\\eta = 0.25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR WORK HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Implementing Neural Networks\n",
    "\n",
    "This question will guide you through the implementation of a fully-connected neural network. Although the question is divided into parts, you need not do the parts in order. In fact, you may want to start with Part C (implementing the neural network itself) so that you see how the pieces fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Loss Functions\n",
    "\n",
    "By subclassing the `Loss` class below, implement the squared-error loss function \n",
    "\n",
    "$$L({\\bf \\hat y}, {\\bf y}) = \\sum_i (\\hat y_i - y_i)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        \"\"\"Calculates the loss as a function of the prediction and the actual.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (float) \n",
    "          The value of the loss for this batch of observations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, predicted, actual):\n",
    "        \"\"\"The derivative of the loss with respect to the prediction.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives of the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class SquaredErrorLoss(Loss):\n",
    "    # TODO: Implement this subclass.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Activation Functions\n",
    "\n",
    "By subclassing the `ActivationFunction` class below, implement the ReLU and Sigmoid activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(object):\n",
    "        \n",
    "    def __call__(self, a):\n",
    "        \"\"\"Applies activation function to the values in a layer.\n",
    "        \n",
    "        Args:\n",
    "          a (np.ndarray, float): the values from the previous layer (after \n",
    "            multiplying by the weights.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The values h = g(a).\n",
    "        \"\"\"\n",
    "        return a\n",
    "    \n",
    "    def derivative(self, h):\n",
    "        \"\"\"The derivatives as a function of the outputs at the nodes.\n",
    "        \n",
    "        Args:\n",
    "          h (np.ndarray, float): the outputs h = g(a) at the nodes.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives dh/da.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "class ReLU(ActivationFunction):\n",
    "    # TODO: Implement this subclass.\n",
    "    pass\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    # TODO: Implement this subclass.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Putting It All Together\n",
    "\n",
    "A `Layer` class has been completely implemented for you. This class applies the activation function to the incoming values and stores the current values at the layer. (We need to store the values because they come in handy when we are doing backpropagation.)\n",
    "\n",
    "The `FullyConnectedNeuralNetwork` class is only partially implemented. You have to implement the `feedforward` and `backprop` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"A data structure for a layer in a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      num_nodes (int): number of nodes in the layer\n",
    "      activation_function (ActivationFunction)\n",
    "      values_pre_activation (np.ndarray, float): most recent values\n",
    "        in layer, before applying activation function\n",
    "      values_post_activation (np.ndarray, float): most recent values\n",
    "        in layer, after applying activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, activation_function=ActivationFunction()):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def get_layer_values(self, values_pre_activation):\n",
    "        \"\"\"Applies activation function to values from previous layer.\n",
    "        \n",
    "        Stores the values (both before and after applying activation \n",
    "        function)\n",
    "        \n",
    "        Args:\n",
    "          values_pre_activation (np.ndarray, float): \n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer before applying the activation function\n",
    "        \n",
    "        Returns: (np.ndarray, float)\n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer after applying the activation function\n",
    "        \"\"\"\n",
    "        self.values_pre_activation = values_pre_activation\n",
    "        self.values_post_activation = self.activation_function(\n",
    "            values_pre_activation\n",
    "        )\n",
    "        return self.values_post_activation\n",
    "\n",
    "        \n",
    "class FullyConnectedNeuralNetwork(object):\n",
    "    \"\"\"A data structure for a fully-connected neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      layers (Layer): A list of Layer objects.\n",
    "      loss (Loss): The loss function to use in training.\n",
    "      learning_rate (float): The learning rate to use in backpropagation.\n",
    "      weights (list, np.ndarray): A list of weight matrices,\n",
    "        length should be len(self.layers) - 1\n",
    "      biases (list, float): A list of bias terms,\n",
    "        length should be equal to len(self.layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, loss, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weight matrices and biases to zeros\n",
    "        self.weights = []\n",
    "        self.biases = [0]\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.weights.append(\n",
    "                np.zeros((self.layers[i - 1].num_nodes, self.layers[i].num_nodes))\n",
    "            )\n",
    "            self.biases.append(0)\n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"Predicts the output(s) for a given set of input(s).\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray, float): A (batch size) x self.layers[0].num_nodes array\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          An array of the predicted output labels, length is the batch size\n",
    "        \"\"\"\n",
    "        # TODO: Implement feedforward prediction.\n",
    "        # Make sure you use Layer.get_layer_values() at each layer to store the values\n",
    "        # for later use in backpropagation.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, predicted, actual):\n",
    "        \"\"\"Updates self.weights and self.biases based on predicted and actual values.\n",
    "        \n",
    "        This will require using the values at each layer that were stored at the\n",
    "        feedforward step.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): An array of the predicted output labels\n",
    "          actual (np.ndarray, float): An array of the actual output labels\n",
    "        \"\"\"\n",
    "        # TODO: Implement backpropagation.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"Trains neural network based on a batch of training data.\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray): A (batch size) x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the batch size.\n",
    "        \"\"\"\n",
    "        predicted = self.feedforward(inputs)\n",
    "        self.backprop(predicted, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Testing It Out\n",
    "\n",
    "Try testing out your neural network implmentation on the simple neural network you considered in Question 1. The following code initializes a neural network with the structure of the network from Question 1. Note that all weights are initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(2), Layer(2, ReLU()), Layer(1)],\n",
    "    learning_rate=0.25\n",
    ")\n",
    "\n",
    "network.train(np.array([[-1, 1]]), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
